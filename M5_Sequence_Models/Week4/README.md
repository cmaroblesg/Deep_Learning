# Transformer Network

## Learning Objectives
* Create positional encodings to capture sequential relationships in data
* Calculate scaled dot-product self-attention with word embeddings
* Implement masked multi-head attention
* Build and train a Transformer model
* Fine-tune a pre-trained transformer model for Named Entity Recognition
* Fine-tune a pre-trained transformer model for Question Answering
* Implement a QA model in TensorFlow and PyTorch
* Fine-tune a pre-trained transformer model to a custom dataset
* Perform extractive Question Answering

## Coding Assignment
* [Transformers Architecture with Tensorflow](./codes/C5_W4_A1_Transformer_Subclass_v1.json)
* [Named-Entity Recognition](./Transformer_application_Named_Entity_Recognition.ipynb)
