# Optimization Algorithms

## Introduction
Develop your deep learning toolbox by adding more advanced optimizations, random minibatching, and learning rate decay scheduling to speed up your models.

## Learning Objectives
* Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
* Use random minibatches to accelerate convergence and improve optimization
* Describe the benefits of learning rate decay and apply it to your optimization

## Programming Assignment
* [Optimization methods](./codes/Optimization_methods.ipynb)
